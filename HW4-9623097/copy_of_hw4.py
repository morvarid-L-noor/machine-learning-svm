# -*- coding: utf-8 -*-
"""Copy of hw4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DrM6aa4nVwBRNvwcmHO82oHdBl19Mj69

**1-a:**
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
from sklearn import svm
from sklearn import metrics
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score
from sklearn.metrics import classification_report, confusion_matrix
# %matplotlib inline

X = np.transpose(np.array([[2, -1],
                           [1, -3],
                           [-1, 3],
                           [-2, 2],
                           [-3, 0],
                           [-1, -1],
                           [-1, -2],#-----------------
                           [4, 1],
                           [4, 4],
                           [1, 4],
                           [-5, 1],
                           [-4.5, 5],
                           [-4, 4],
                           [-4, -2],
                           [-3, -4],
                           [1, -5]]))
y = np.array([1,1,1,1,1,1,1,-1,-1,-1,-1,-1,-1,-1,-1,-1])

X_test = np.transpose(np.array([[1, 1],[1, 2],[4, -4],[5, -2]]))
y_test =  np.array([1,1,-1,-1])

def define_y(x):
    y=[]
    for i in range(np.shape(x)[0]):
        if(x[i][0])<1:
            y.append(-1)
        else:
            y.append(1)
    return y

#train matrix : first element of each row = r  ,  second element in each row = theta
polar_train=np.random.rand(100,2)*[2,np.pi*2]
polar_test=np.random.rand(20,2)*[2,np.pi*2]
y_train=labeling(polar_train)
y_test=labeling(polar_test)
train=np.transpose([polar_train[:,0]*np.cos(polar_train[:,1]),polar_train[:,0]*np.sin(polar_train[:,1])])
test=np.transpose([polar_test[:,0]*np.cos(polar_test[:,1]),polar_test[:,0]*np.sin(polar_test[:,1])])

linear_clf=svm.LinearSVC()
linear_clf.fit(train,y_train)

x=np.linspace(-3,3,30)
y=np.linspace(-3,3,30)
xx,yy=np.meshgrid(x,y)
xy=np.vstack([np.ravel(xx),np.ravel(yy)]).T
z=linear_clf.decision_function(xy).reshape(30,30)
plt.contour(xx,yy,z,levels=1,colors='k')
plt.scatter(train[:,0],train[:,1],c=y_train,s=60)
#plt.scatter(linear_clf.support_vectors_[:,0],linear_clf.support_vectors_[:,1],linewidths=1.5,edgecolors='k',facecolor='none',s=75)
plt.xlim(-2,2)
plt.ylim(-2,2)

y_train=define_y(train)
y_test=define_y(test)

model=svm.LinearSVC()
model.fit(train,y_train)

print("train accuracy : ",metrics.accuracy_score(y_train, model.predict(train)))
print("test accuracy : ",metrics.accuracy_score(y_test, model.predict(test)))

x=np.linspace(-3,3,30)
y=np.linspace(-3,3,30)
xx,yy=np.meshgrid(x,y)
xy=np.vstack([np.ravel(xx),np.ravel(yy)]).T
z=model.decision_function(xy).reshape(30,30)
plt.contour(xx,yy,z,levels=1)
plt.scatter(train[:,0],train[:,1],c=y_train,s=60)
plt.xlim(-2,2)
plt.ylim(-2,2)

sv_index=np.where(np.abs(model.decision_function(train))<.03)[0]
for i in sv_index:
    plt.scatter(train[i][0],train[i][1],facecolors='none',s=80,linewidths=3)
plt.scatter(train[:,0],train[:,1],c=y_train,s=60)
plt.contour(xx,yy,z,levels=1)

"""**1-b :**"""

from sklearn.model_selection import cross_val_score
ac=[]
c = []
for i in range(1,200):
    soft_model=svm.SVC(kernel='linear',C=i/100)
    acc=np.mean(cross_val_score(soft_model,train,y_train))
    ac.append(acc)
    c.append(i/100)

import matplotlib.pyplot as plt
plt.plot(c,ac)

"""so the best value for c is 1"""

soft_model=svm.SVC(kernel='linear',C=1)
soft_model.fit(train,y_train)

print("train accuracy :",metrics.accuracy_score(y_train,soft_model.predict(train)))
print("test accuracy",metrics.accuracy_score(y_test,soft_model.predict(test)))

z=soft_model.decision_function(xy).reshape(30,30)
plt.contour(xx,yy,z,levels=[-1,0,1],colors='k',linestyles=['--'])
plt.scatter(train[:,0],train[:,1],c=y_train,s=60,cmap='cool')
plt.scatter(soft_model.support_vectors_[:,0],soft_model.support_vectors_[:,1],linewidths=2,facecolor='none',s=75)

"""**1-c :**"""

# accuarcy of cross validation on rbf kernel
np.mean(cross_val_score(svm.SVC(kernel='rbf'),train,y_train))

# accuarcy of cross validation on  polynomial kernel with degree 2
poly_2_clf=
np.mean(cross_val_score(svm.SVC(kernel='poly',degree=2),train,y_train))

# accuarcy of cross validation on polynomial kernel with degree 3
np.mean(cross_val_score(svm.SVC(kernel='poly',degree=3),train,y_train))

# most accuracy belongs to polynomial kernel with degree 3 so :
best_model = svm.SVC(kernel='poly',degree=3)
best_model.fit(train,y_train)

print(" train accuracy : ",metrics.accuracy_score(y_train,best_model.predict(train)))
print(" test accuracy : ",metrics.accuracy_score(y_test,best_model.predict(test)))

z=poly_2_clf.decision_function(xy).reshape(30,30)
plt.contour(xx,yy,z,levels=[-1,0,1],linestyles=['--'])
plt.scatter(train[:,0],train[:,1],c=y_train,s=60,cmap='cool')
plt.scatter(poly_2_clf.support_vectors_[:,0],poly_2_clf.support_vectors_[:,1],linewidths=2,facecolor='none',s=75)

"""*  **2-a :**"""

iris=pd.read_csv('/content/iris .csv')

x=iris.iloc[:,:-1]
y=iris.iloc[:,4]
x_train,x_test, y_train, y_test=train_test_split(x,y,test_size=0.30)

model=SVC()
model.fit(x_train, y_train)
# testing
pred=model.predict(x_test)
print(confusion_matrix(y_test,pred))
print(classification_report(y_test, pred))

y_train.shape

# validation
x_train,x_valid, y_train, y_valid=train_test_split(x_train,y_train,test_size=0.25)

model=SVC()
model.fit(x_train, y_train)
pred=model.predict(x_valid)
print(confusion_matrix(y_valid,pred))
print(classification_report(y_valid, pred))

"""so accuracy on testset is " 87 % " and accuracy on validationset is " 88 % "

*  **2-b :**
"""

ac = []
c = []
x0_train,x0_test, y0_train, y0_test=train_test_split(x,y,test_size=0.30)
x0_train , x0_valid , y0_train , y0_valid = train_test_split(x0_train,y0_train,test_size=0.25)
for i in range(1 , 200):
  svclassifier = SVC(kernel='linear', C = i/100)
  svclassifier.fit(x0_train, y0_train)
  y0_pred = svclassifier.predict(x0_valid)
  c.append(i/100)
  ac.append(accuracy_score(y0_valid, y0_pred))

import matplotlib.pyplot as plt
plt.plot(c,ac)

"""**according to the plot the best c-value for soft svm is from 0.5 to 1.12 which gives us the accuracy of 96% 
so we use c=1 for this part**
"""

x0_train,x0_test, y0_train, y0_test=train_test_split(x,y,test_size=0.30)
x0_train , x0_valid , y0_train , y0_valid = train_test_split(x0_train,y0_train,test_size=0.25)
svclassifier = SVC(kernel='linear', C = 1)
svclassifier.fit(x0_train, y0_train)

# validation accuracy
y0_pred = svclassifier.predict(x0_valid)
print(classification_report(y0_valid, y0_pred))

# test accuracy
y00_pred = svclassifier.predict(x0_test)
print(classification_report(y0_test, y00_pred))

"""*  **2-c :**"""

svclassifier = SVC(kernel='poly', degree=8)
x1_train,x1_test, y1_train, y1_test=train_test_split(x,y,test_size=0.30)
x1_train , x1_valid , y1_train , y1_valid = train_test_split(x1_train,y1_train,test_size=0.25)
svclassifier.fit(x1_train, y1_train)
y1_pred = svclassifier.predict(x1_valid)

print(confusion_matrix(y1_valid, y1_pred))
print(classification_report(y1_valid, y1_pred))

"""so validation accuracy by using polynomial kernel is 96%"""

svclassifier = SVC(kernel='rbf')
x2_train,x2_test, y2_train, y2_test=train_test_split(x,y,test_size=0.30)
x2_train , x2_valid , y2_train , y2_valid = train_test_split(x2_train,y2_train,test_size=0.25)
svclassifier.fit(x2_train, y2_train)
y2_pred = svclassifier.predict(x2_valid)

print(confusion_matrix(y2_valid, y2_pred))
print(classification_report(y2_valid, y2_pred))

""" validation accuracy by using rbf kernel is 100%

  **so the rbf kernel is better then the otherone**
"""

# we've seen the accuracy on trainigset (validation) so now is testset's turn :
y22_pred = svclassifier.predict(x2_test)
print(confusion_matrix(y2_test, y22_pred))
print(classification_report(y2_test, y22_pred))

"""so for rbf kernel we have 45 soppurt vectors and 98 % accuracy on testset

**2-d :**
"""

ac3 = []
c3 = []
x3_train,x3_test, y3_train, y3_test=train_test_split(x,y,test_size=0.30)
x3_train , x3_valid , y3_train , y3_valid = train_test_split(x3_train,y3_train,test_size=0.25)
for i in range(1 , 100):
  svclassifier = SVC(kernel='rbf', C = i/100)
  svclassifier.fit(x3_train, y3_train)
  y3_pred = svclassifier.predict(x3_valid)
  c3.append(i/100)
  ac3.append(accuracy_score(y3_valid, y3_pred))

import matplotlib.pyplot as plt
plt.plot(c3,ac3)

"""according to plot we use c = 0.5"""

x3_train,x3_test, y3_train, y3_test=train_test_split(x,y,test_size=0.30)
x3_train , x3_valid , y3_train , y3_valid = train_test_split(x3_train,y3_train,test_size=0.25)
svclassifier = SVC(kernel='linear', C = 0.5)
svclassifier.fit(x3_train, y3_train)

# validation accuracy
y3_pred = svclassifier.predict(x3_valid)
print(classification_report(y3_valid, y3_pred))

# test accuracy
y33_pred = svclassifier.predict(x3_test)
print(classification_report(y3_test, y33_pred))